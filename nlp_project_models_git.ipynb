{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4A5lvIf99Bw1"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T02:46:41.637164Z",
     "start_time": "2020-05-06T02:46:41.632338Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Nl9s6sFj9Bw2",
    "outputId": "43426280-5c2d-4298-bb2b-d2ffb77d53c9"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline\n",
    "import random\n",
    "random.seed(30)\n",
    "from sklearn.utils import compute_sample_weight\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzAnmp3T9Bw6"
   },
   "outputs": [],
   "source": [
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)    \n",
    "\n",
    "def save_object(filename, object):\n",
    "    with open(filename, 'wb') as filehandler:\n",
    "        pickle.dump(object, filehandler)\n",
    "        print('Saved to ' ,filename)\n",
    "def read_object(filename):\n",
    "    with open(filename, 'rb') as filehandler:\n",
    "        pickle.load(filehandler)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyEKvHZ79Bw8"
   },
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     30
    ],
    "colab": {},
    "colab_type": "code",
    "id": "DK8QXM8z9Bw8"
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    from sklearn.utils import Bunch\n",
    "    def decodeText(text):\n",
    "        if type(text) != str:\n",
    "            try:\n",
    "                text = text.decode('utf-8')\n",
    "            except:\n",
    "                text = text.decode('latin-1')\n",
    "        text = text.rstrip()\n",
    "        return text.split('@')\n",
    "    \n",
    "    contexts, labels = [], []\n",
    "    with open(filename, 'rb') as f:\n",
    "        text = f.readlines()\n",
    "    \n",
    "    for t in tqdm(text):\n",
    "        context, label = decodeText(t)\n",
    "        contexts += [context]\n",
    "        if label == 'positive':\n",
    "            target = 1\n",
    "        elif label == 'negative':\n",
    "            target = -1\n",
    "        elif label == 'neutral':\n",
    "            target = 0\n",
    "        else:\n",
    "            raise('ERROR: ' + label)\n",
    "        labels += [target]\n",
    "        \n",
    "#    return Bunch(contexts = contexts, labels = np.array(labels))\n",
    "    return pd.DataFrame(zip(contexts, labels), columns = ['contexts','labels'])\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - data is a dataframe with columns contexts as series of text\n",
    "    output:\n",
    "        - preprocessed dataframe\n",
    "    \"\"\"\n",
    "    import string\n",
    "    import re\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    # lowercase \n",
    "    data.contexts = data.contexts.apply(lambda x: ' '.join(word.lower() for word in x.split()))\n",
    "    \n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"don't\", \"do not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"aren't\", \"are not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"couldn't\", \"could not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"didn't\", \"did not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"doesn't\", \"does not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"hadn't\", \"had not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"hasn't\", \"has not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"haven't\", \"have not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"isn't\", \"is not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"mightn't\", \"might not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"mustn't\", \"must not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"needn't\", \"need not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"shan't\", \"shall not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"wasn't\", \"was not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"weren't\", \"were not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"won't\", \"will not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"wouldn't\", \"would not\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"shouldnt\", \"should not\", x))\n",
    "    \n",
    "    # remove numbers\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\d+\", '', x))\n",
    "    # remove punctuations\n",
    "    # data.contexts = data.contexts.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"what's\", \"what is \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\'s\", \" \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\'ve\", \" have \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"i'm\", \"i am \",  x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\'re\", \" are \",  x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\'d\", \" would \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"!\", \" ! \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\/\", \" \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\^\", \" ^ \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\+\", \" + \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\-\", \" - \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\":\", \" : \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\" e g \", \" eg \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\" b g \", \" bg \",x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\" u s \", \" american \", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\0s\", \"0\",  x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\" 9 11 \", \"911\",  x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"e - mail\", \"email\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"j k\", \"jk\", x))\n",
    "    data.contexts = data.contexts.apply(lambda x: re.sub(r\"\\s{2,}\", \" \", x))\n",
    "\n",
    "    # remove whitespace\n",
    "    data.contexts = data.contexts.apply(lambda x: x.strip())\n",
    "    # tokenization\n",
    "    data.contexts = data.contexts.apply(word_tokenize)\n",
    "    # remove stop words\n",
    "    #stop_words = stopwords.words('english')\n",
    "    #for word in ['above', 'below','up','down', 'more', 'not']:\n",
    "    #    stop_words.remove(word)\n",
    "    #data.contexts = data.contexts.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    # stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data.contexts = data.contexts.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUdS6B279Bw_",
    "outputId": "e3993753-33f9-4366-9615-d96e0eaf1b3d"
   },
   "outputs": [],
   "source": [
    "data3 = load_data('Sentences_AllAgree.txt')\n",
    "preprocessing(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBaIGvR19BxB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data3.contexts, data3.labels,test_size = 0.2, shuffle = True, random_state = 1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test,test_size = 0.5, shuffle = True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YFNaRwM9BxF",
    "outputId": "9ac6dd37-fc5b-4935-a6a8-c5a9efdf3bd9"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cat_train = to_categorical(y_train, num_classes=3)\n",
    "y_cat_dev = to_categorical(y_dev, num_classes=3)\n",
    "y_cat_test = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olsXn-mw9BxH",
    "outputId": "7b128eee-12a6-46d9-d665-b8f1a0b1e300"
   },
   "outputs": [],
   "source": [
    "print('max sentence length')\n",
    "[np.max(data3.contexts.apply(lambda x: len(x)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "eDRslHLn9BxJ"
   },
   "source": [
    "## EAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T03:03:20.971124Z",
     "start_time": "2020-05-06T03:03:20.962575Z"
    },
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "u5QMLqCD9BxK"
   },
   "outputs": [],
   "source": [
    "def EDA(data):\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    wc = WordCloud().generate(' '.join(word for text in data.contexts for word in text))\n",
    "    fig, [ax0, ax1] = plt.subplots(nrows = 1, ncols = 2, figsize = [10,4])\n",
    "    ax0.imshow(wc)\n",
    "    ax0.axis('off')\n",
    "    \n",
    "    pd.Series(data.labels).value_counts().plot(kind = 'bar', axes = ax1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "58rsjsP59BxL",
    "outputId": "4c023bb1-a667-4c4e-9d7a-7190d773d119"
   },
   "outputs": [],
   "source": [
    "EDA(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "k6QZlg9H9BxN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                        min_df=5, \n",
    "                        norm='l2', \n",
    "                        encoding='latin-1', \n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "features = tfidf.fit_transform(data3.contexts.apply(lambda x: ' '.join(x))).toarray()\n",
    "labels = data3.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "iiMeyKUp9BxQ",
    "outputId": "3b87a277-84d4-40b8-8cbe-4eb2d6f42267"
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "for category_id in sorted(labels.unique()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"------------- Category {} ------------\".format(category_id))\n",
    "    print(\"Most correlated unigrams:\")\n",
    "    print(unigrams[-N:])\n",
    "    print(\"Most correlated bigrams:\")\n",
    "    print(bigrams[-N:])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BCMpbXE9BxV"
   },
   "source": [
    "## Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     45
    ],
    "colab": {},
    "colab_type": "code",
    "id": "F_65SPzr9BxV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def model_eval(model, X_train, X_dev, X_test, y_train, y_dev, y_test, name = 'Model'):\n",
    "    print('--------------------------------------  {} --------------------------------------'.format(name))\n",
    "    F1 = [model.score(X_train, y_train), model.score(X_dev, y_dev), model.score(X_test, y_test)]\n",
    "    acc = [model.score(X_train, y_train, method = 'accuracy'), model.score(X_dev, y_dev, method = 'accuracy'), model.score(X_test, y_test, method = 'accuracy')]\n",
    "    print(\"Training set F1score: \",F1[0]) \n",
    "    print(\"Dev set F1score: \", F1[1])\n",
    "    print(\"Test set F1score: \", F1[2])\n",
    "    \n",
    "    def gen_confusion_matrix(pred_label, y_labels):\n",
    "        conf_mat = confusion_matrix(y_labels, pred_label)\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        df = pd.DataFrame(conf_mat, index = ['actual_negative','actual_neutral','actual_positive'], columns = ['pred_negative','pred_neutral','pred_positive']).style.background_gradient(cmap=cm)\n",
    "        count = np.sum(conf_mat, axis = 1)\n",
    "        precision = np.diag(conf_mat) / np.sum(conf_mat, axis = 0)\n",
    "        recall = np.diag(conf_mat) / np.sum(conf_mat, axis = 1)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        #weighted_avg = [np.average(precision, weights = count), np.average(recall, weights = count), np.average(f1score, weights = count)]\n",
    "        macro_avg = [np.average(precision), np.average(recall), np.average(f1score)]\n",
    "\n",
    "        result = pd.DataFrame([precision, recall, f1score, count], index = ['precision','recall','f1score','count'], columns = ['negative','neutral','positive']).T\n",
    "        macro_avg = pd.Series(macro_avg + [np.NaN], result.columns )\n",
    "        #weighted_avg = pd.Series(weighted_avg + [np.NaN], result.columns )\n",
    "        result.loc['macro_avg',:] = macro_avg\n",
    "        #result.loc['weighted_avg',:] = weighted_avg\n",
    "        result = result.round(2).style.highlight_max( axis = 0, color = 'green')\n",
    "    \n",
    "        return result, df\n",
    "\n",
    "    result, df = gen_confusion_matrix(model.predict(X_train), y_train)\n",
    "    print('------------  Train Confusion Matirx ------------')\n",
    "    display_side_by_side(result.render(), df.render())    \n",
    "    result, df = gen_confusion_matrix(model.predict(X_dev), y_dev)\n",
    "    print('------------  Dev Confusion Matirx ------------')\n",
    "    display_side_by_side(result.render(), df.render())\n",
    "    result, df = gen_confusion_matrix(model.predict(X_test), y_test)\n",
    "    print('------------  Test Confusion Matirx ------------')\n",
    "    display_side_by_side(result.render(), df.render())\n",
    "    return F1 + acc\n",
    "def performance_compare_plot(perf):\n",
    "\n",
    "    metrics = ['train_f1','val_f1','test_f1']\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = [5 * 3, 4], sharey = True)\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = perf[['model_name','feature_method', metric]]\n",
    "        sns.boxplot( x= 'model_name', y = metric, data = data, ax = axes[idx])\n",
    "        chart = sns.stripplot(x = 'model_name', y = metric, data = data, size=8, jitter=True, edgecolor=\"gray\", linewidth=2, ax = axes[idx])\n",
    "        axes[idx].set_xticklabels(chart.get_xticklabels(), rotation=45, color = 'white')\n",
    "        axes[idx].set_title(metric, color = 'white')\n",
    "        axes[idx].tick_params(labelcolor = 'white')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWcmD86x9BxX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     15
    ],
    "colab": {},
    "colab_type": "code",
    "id": "dU55_Dcm9BxZ"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, feature_method, model = LogisticRegression(C=1.0), min_feature_ct = 10):\n",
    "        # min_feature_ct: int, ignore the features appear less than this number to avoid overfitting\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_ct = min_feature_ct\n",
    "        #self.L2_reg = L2_reg\n",
    "        self.model = model\n",
    "    def pipeline(self, X, training = False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            - X: featurized input\n",
    "        output:\n",
    "            2d sparse matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build feature_vocab during training\n",
    "        if training:\n",
    "            fea_ct = Counter([word for sent in X for word in self.feature_method(sent)])\n",
    "            fea_ct = {key: val for key, val in fea_ct.items() if val >= self.min_feature_ct}\n",
    "\n",
    "            self.word2idx = {key: idx + 1 for idx, key in enumerate(list(fea_ct.keys()))}\n",
    "            #idx2word = {idx + 1: key for idx, key in enumerate(list(fea_ct.keys()))}\n",
    "            self.word2idx['_unknown_'] = 0\n",
    "            #idx2word[0] = '_unknown_'\n",
    "\n",
    "        #text_encoded = [[self.word2idx.get(word, 0) for word in self.feature_method(sent)] for sent in X]\n",
    "        #print(text_encoded)\n",
    "        sp_matrix = dok_matrix((len(X), len(self.word2idx)))\n",
    "        for nrow, sent in enumerate(X):\n",
    "            feat = self.feature_method(sent)\n",
    "            for key, val in feat.items():\n",
    "                # print(nrow, key, val)\n",
    "                sp_matrix[nrow, self.word2idx.get(key, 0 )] = val\n",
    "        return sp_matrix    \n",
    "    def fit(self, X, y):\n",
    "        X= self.pipeline(X, training = True)\n",
    "        weight_dict = compute_class_weight(y)\n",
    "        self.model.fit(X,y, sample_weight = [weight_dict.get(label) for label in y])\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        X = self.pipeline(X, training = False)\n",
    "        return self.model.predict(X)\n",
    "    def score(self, X, y, method = 'F1-score'):\n",
    "        pred_y = self.predict(X)\n",
    "        if method == 'F1-score':\n",
    "            #X= self.pipeline(X, training = False)\n",
    "            #return self.model.score(X, y) \n",
    "            #weight_dict = compute_class_weight(y)\n",
    "            #return f1_score(y, pred_y, sample_weight = [weight_dict.get(label) for label in y])\n",
    "            return f1_score(y, pred_y, average = 'macro')\n",
    "        elif method == 'accuracy':\n",
    "            X= self.pipeline(X, training = False)\n",
    "            return self.model.score(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11,
     17
    ],
    "colab": {},
    "colab_type": "code",
    "id": "u-asl71E9Bxa"
   },
   "outputs": [],
   "source": [
    "def BOW_featurizer(text):\n",
    "    from collections import Counter\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - text: a list of tokens (str or int)\n",
    "    output:\n",
    "        dictionary of {token: count}\n",
    "    \"\"\"\n",
    "    return Counter(text)    \n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams, ngrams\n",
    "def everyGram(text, N = 5):\n",
    "    return Counter(list(everygrams(list(pad_sequence(text,\n",
    "            pad_left = True, left_pad_symbol = '<s>',\n",
    "            pad_right = True, right_pad_symbol = '<s>',\n",
    "            n= N)),\n",
    "            max_len = N))) \n",
    "def NGram(text, N = 5):\n",
    "    return Counter(list(ngrams(list(pad_sequence(text,\n",
    "        pad_left = True, left_pad_symbol = '<s>',\n",
    "        pad_right = True, right_pad_symbol = '<s>',\n",
    "        n= N)),\n",
    "        n = N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3yobd309Bxc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "j6VyrY4z9Bxe"
   },
   "outputs": [],
   "source": [
    "def compute_class_weight(y):\n",
    "    \"\"\"\n",
    "    n_samples / (n_classes * n_samples_with_class)\n",
    "    \"\"\"\n",
    "    ct = Counter(y)\n",
    "    n_classes = len(ct.keys())\n",
    "    n_samples = np.sum(np.array(list(ct.values())))\n",
    "    weights = n_samples / (n_classes * np.array(list(ct.values())))\n",
    "    weight = {}\n",
    "    for i, label in enumerate(ct.keys()):\n",
    "        weight[label] = weights[i]\n",
    "\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNjcWj6P9Bxg"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1.0)\n",
    "\n",
    "model_list = {'randomForest':RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "          'LinearSVC':LinearSVC(),\n",
    "          'MultinomialNB':MultinomialNB(  ),\n",
    "          'LogisticRegression':LogisticRegression(C=1.0, tol=1e-4, random_state=0, solver='lbfgs', multi_class='auto'),\n",
    "             }\n",
    "# , class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uM5VOIW9Bxi"
   },
   "source": [
    "#### textfile4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGI6askE9Bxi",
    "outputId": "6c5b4a1b-bdb5-4007-be97-5bf7f77cf4a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_method_list = {'BOW': BOW_featurizer, 'everyGram':everyGram}\n",
    "perf = []\n",
    "for model_name, model in model_list.items():\n",
    "    print('---------------------------------------------- {} ------------------------------------------------'.format(model_name))\n",
    "    for feature_method_name, feature_method in feature_method_list.items():\n",
    "        cls = SentimentClassifier(model = model, feature_method=feature_method, min_feature_ct = 10)\n",
    "        cls = cls.fit(X_train, y_train)\n",
    "        train_f1 , val_f1 , test_f1, train_acc , val_acc , test_acc  = model_eval(cls, X_train, X_dev, X_test, y_train, y_dev, y_test, name = feature_method_name)\n",
    "        perf += [[model_name, feature_method_name,train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 ]]\n",
    "perf = pd.DataFrame(perf, columns = ['model_name','feature_method','train_acc','train_f1','val_acc','val_f1','test_acc','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCYGWq5a9Bxk",
    "outputId": "b9c5c314-293a-4c34-d14a-2dc4c3ea9109"
   },
   "outputs": [],
   "source": [
    "performance_compare_plot(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_BMyZ8Y9Bxm",
    "outputId": "3341ecbe-c36c-45d0-b038-f311e724c126"
   },
   "outputs": [],
   "source": [
    "perf['model_name'] = perf['model_name'] + '_' + perf['feature_method']\n",
    "perf.pop('feature_method')\n",
    "perf.set_index('model_name').style.highlight_max( axis = 0, color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qARbUPD09Bxn"
   },
   "outputs": [],
   "source": [
    "perf.to_csv('perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPJidQ-t9Bxp"
   },
   "outputs": [],
   "source": [
    "# tmp_perf = pd.read_csv('perf.csv', index_col = 0)\n",
    "tmp_perf = tmp_perf.sort_values('test_f1', ascending = False)\n",
    "#.style.hightlight_max(axis = 0, color = 'green')\n",
    "tmp_perf.set_index('model_name').style.highlight_max( axis = 0, color = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O03qz1EW9Bxs"
   },
   "outputs": [],
   "source": [
    "tmp = tmp_perf[:4].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQXHEJcr9Bxu"
   },
   "outputs": [],
   "source": [
    "tmp.reset_index()['model_name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHpLXo1M9Bxw"
   },
   "outputs": [],
   "source": [
    "tmp.model_name = ['BI_LSTM_auc', 'LogReg_everyGram', 'CNN_acc',\n",
    "       'LogReg_BOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATDl5PPu9Bxx"
   },
   "outputs": [],
   "source": [
    "performance_compare_plot(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijw9JUgu9Bx0"
   },
   "source": [
    "## Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPd7LbUC9Bx0",
    "outputId": "ca9bff8a-03f8-459a-f6e6-1a0a1ff6bce7"
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import EarlyStopping\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDbs7ahd9Bx1"
   },
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "max_vocab_size = 20000\n",
    "max_sent_length = 109  # changed in version 2\n",
    "embedding_dim = 100  \n",
    "max_sent_length3 = 80   # newly added in version 2\n",
    "hidden_dim = 256\n",
    "n_epoches = 100\n",
    "min_token_ct = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQwnMczC9Bx3"
   },
   "source": [
    "### Create sequence\n",
    "max_vocab_size = 20000\n",
    "max_sent_length = 109  # note this difference here from the previous version\n",
    "max_sent_length3 = 80  # newly added\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_epoches = 100\n",
    "min_token_ct = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "k0uqJwy09Bx4"
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, min_token_ct):\n",
    "        print('Initialize tokenizer')\n",
    "        self.word_index = {}\n",
    "        self.min_token_ct = min_token_ct\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.UNK='unknown'\n",
    "        self.PAD='_pad_'\n",
    "    def fit_on_texts(self,X):\n",
    "        \"\"\"\n",
    "        For given training data, list of vocabulary list, i.g.\n",
    "        [[\"this\", \"set\", \"1\"],\n",
    "         [\"this\", \"is\", \"another\", \"set\"],\n",
    "         ]\n",
    "\n",
    "        return the vocab list and rev_vocab dictionary\n",
    "        2 numerical encodings are reserved: {PAD:0,UNK:1}\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        token_ct = Counter([word for sent in X for word in sent])\n",
    "        token_ct = {k: v for k, v in token_ct.items() if v >= self.min_token_ct}\n",
    "        vocab = sorted(token_ct, key=token_ct.get, reverse=True)\n",
    "        vocab = [self.PAD,self.UNK] + vocab\n",
    "\n",
    "        word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "        self.vocab = vocab\n",
    "        self.word_index = word2idx\n",
    "    def texts_to_sequences(self,X):\n",
    "        \"\"\"\n",
    "        For given training data, list of vocabulary list, i.g.\n",
    "        [[\"this\", \"set\", \"1\"],\n",
    "         [\"this\", \"is\", \"another\", \"set\"],\n",
    "        ]\n",
    "        \"\"\"\n",
    "        assert(len(self.word_index) > 0 )\n",
    "        UNK_idx = self.word_index.get(self.UNK)\n",
    "        return [[self.word_index.get(word, UNK_idx) for word in sent] for sent in X ]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6fpcqJK9Bx5",
    "outputId": "d113be93-738c-434b-b668-590aaccd35fd"
   },
   "outputs": [],
   "source": [
    "tokenizer3 = Tokenizer(min_token_ct)\n",
    "tokenizer3.fit_on_texts(X_train)\n",
    "vocabulary_size3 = len(tokenizer3.vocab) \n",
    "\n",
    "print([ vocabulary_size3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFRo1VJB9Bx7"
   },
   "outputs": [],
   "source": [
    "X_cod_train = pad_sequences(tokenizer3.texts_to_sequences(X_train), maxlen=max_sent_length3, padding = 'post' )\n",
    "X_cod_dev = pad_sequences(tokenizer3.texts_to_sequences(X_dev), maxlen=max_sent_length3, padding = 'post' )\n",
    "X_cod_test = pad_sequences(tokenizer3.texts_to_sequences(X_test), maxlen=max_sent_length3, padding = 'post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     15,
     73
    ],
    "colab": {},
    "colab_type": "code",
    "id": "qJQ754ZG9Bx8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def bias_var_tradeoff_plot(history):\n",
    "    metrics = list(history.history.keys())\n",
    "    length = len(metrics)\n",
    "    metrics = metrics[- int(length / 2):]\n",
    "    length = len(metrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = length, figsize = [5 * length,4])\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_title(metrics[i], color = 'white')\n",
    "        ax.plot(history.history[metrics[i]], label='train')\n",
    "        ax.plot(history.history['val_' +metrics[i]], label='validation')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def model_eval(model, history, X_train, y_train_labels, X_dev, y_dev_labels, X_test, y_test, y_test_labels, model_name = None):\n",
    "    test_accr = model.evaluate(X_test, y_test)\n",
    "    test_acc = test_accr[1]\n",
    "    print('\\n')\n",
    "    print('----------------------- {}: Test_Loss: {:0.3f} Test_Accuracy: {:0.3f} -----------------------'.format(model_name, test_accr[0],test_accr[1]))\n",
    "    \n",
    "    #accuracy = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], test_accr[1]]\n",
    "    #loss = [history.history['loss'][-1], history.history['val_loss'][-1], test_accr[0]]\n",
    "    \n",
    "\n",
    "    def gen_confusion_matrix(y_pred, y_labels):\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import seaborn as sns\n",
    "        pred_label = []\n",
    "        for pred in y_pred:\n",
    "            if np.argmax(pred) == 0:\n",
    "                pred_label += [0]\n",
    "            elif np.argmax(pred) == 1:\n",
    "                pred_label += [1]\n",
    "            elif np.argmax(pred) == 2:\n",
    "                pred_label += [-1]\n",
    "\n",
    "\n",
    "        conf_mat = confusion_matrix(y_labels, pred_label)\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        df = pd.DataFrame(conf_mat, index = ['actual_negative','actual_neutral','actual_positive'], columns = ['pred_negative','pred_neutral','pred_positive']).style.background_gradient(cmap=cm)\n",
    "        print('Confusion Matirx')\n",
    "\n",
    "\n",
    "        count = np.sum(conf_mat, axis = 1)\n",
    "        precision = np.diag(conf_mat) / np.sum(conf_mat, axis = 0)\n",
    "        recall = np.diag(conf_mat) / np.sum(conf_mat, axis = 1)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        weighted_avg = [np.average(precision, weights = count), np.average(recall, weights = count), np.average(f1score, weights = count)]\n",
    "        macro_avg = [np.average(precision), np.average(recall), np.average(f1score)]\n",
    "\n",
    "        result = pd.DataFrame([precision, recall, f1score, count], index = ['precision','recall','f1score','count'], columns = ['negative','neutral','positive']).T\n",
    "        macro_avg = pd.Series(macro_avg + [np.NaN], result.columns )\n",
    "        #weighted_avg = pd.Series(weighted_avg + [np.NaN], result.columns )\n",
    "        result.loc['macro_avg',:] = macro_avg\n",
    "        #result.loc['weighted_avg',:] = weighted_avg\n",
    "        result = result.round(2).style.highlight_max( axis = 0, color = 'green')\n",
    "        \n",
    "        return result, df, macro_avg[2]\n",
    "    print('------------  Train Confusion Matirx ------------')\n",
    "    result, df, train_f1 = gen_confusion_matrix(model.predict(X_train), y_train_labels)\n",
    "    display_side_by_side(result.render(), df.render())\n",
    "    print('------------  Dev Confusion Matirx ------------')\n",
    "    result, df, dev_f1 = gen_confusion_matrix(model.predict(X_dev), y_dev_labels)\n",
    "    display_side_by_side(result.render(), df.render())\n",
    "    print('------------  Test Confusion Matirx ------------')\n",
    "    result, df, test_f1 = gen_confusion_matrix(model.predict(X_test), y_test_labels)\n",
    "    display_side_by_side(result.render(), df.render())\n",
    "    \n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    return train_acc, train_f1 , val_acc, dev_f1 , test_acc, test_f1\n",
    "def performance_compare_plot(perf):\n",
    "    import seaborn as sns\n",
    "    metrics = ['train_acc','val_acc','test_acc']\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = [5 * 3, 4], sharey = True)\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = perf[['model_name',metric]]\n",
    "        sns.boxplot( x= 'model_name', y = metric, data = data, ax = axes[idx])\n",
    "        chart = sns.stripplot(x = 'model_name', y = metric, data = data, size=8, jitter=True, edgecolor=\"gray\", linewidth=2, ax = axes[idx])\n",
    "        axes[idx].set_xticklabels(chart.get_xticklabels(), rotation=60, color = 'white')\n",
    "        axes[idx].set_title(metric, color = 'white')\n",
    "        axes[idx].tick_params(labelcolor = 'white')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    metrics = ['train_f1','val_f1','test_f1']\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = [5 * 3, 4], sharey = True)\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        data = perf[['model_name', metric]]\n",
    "        sns.boxplot( x= 'model_name', y = metric, data = data, ax = axes[idx])\n",
    "        chart = sns.stripplot(x = 'model_name', y = metric, data = data, size=8, jitter=True, edgecolor=\"gray\", linewidth=2, ax = axes[idx])\n",
    "        axes[idx].set_xticklabels(chart.get_xticklabels(), rotation=60, color = 'white')\n",
    "        axes[idx].set_title(metric, color = 'white')\n",
    "        axes[idx].tick_params(labelcolor = 'white')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esuqFTFq9Bx-"
   },
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "546qYGkc9Bx_",
    "outputId": "d8f8ee6f-2deb-4eff-c213-be4d3425933d"
   },
   "outputs": [],
   "source": [
    "glove = pd.read_csv(\"glove_6B_100d_top100k.csv\"); glove.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "oRlmO-wW9ByA",
    "outputId": "1b2cdb0d-8bf2-4818-ff75-a47f4fe060e1"
   },
   "outputs": [],
   "source": [
    "unseen_word_list = []\n",
    "embedding_matrix3 = np.zeros((vocabulary_size3, 100))\n",
    "for word, index in tokenizer3.word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix3[index] = embedding_vector\n",
    "    else:\n",
    "        unseen_word_list += [word]\n",
    "print('{}/{}'.format(len(unseen_word_list), len(tokenizer3.word_index)))\n",
    "print(unseen_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uuNC0FO79ByC",
    "outputId": "5dedad37-f529-4c96-dd16-04c3b4e08d5b"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "METRICS = [\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(curve=\"PR\", name='auc'),\n",
    "    'accuracy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "8vj4nqOu9ByE"
   },
   "outputs": [],
   "source": [
    "## Network architecture\n",
    "def lstm(vocabulary_size, embedding_dim,hidden_dim, max_sent_length, embedding_matrix ):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_sent_length,weights = [embedding_matrix], trainable=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,5,activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 4))\n",
    "    model.add(LSTM(hidden_dim))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= METRICS)\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "## Network architecture\n",
    "def CNN(vocabulary_size, embedding_dim, hidden_dim, max_sent_length, embedding_matrix ):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_sent_length,weights = [embedding_matrix], trainable=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(100,5,activation = 'tanh'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= METRICS)\n",
    "\n",
    "   #model.add(Conv1D(filters=100, kernel_size=3, activation=\"tanh\", name=\"Conv1D-1\"))\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "## Network architecture\n",
    "def bi_lstm(vocabulary_size, embedding_dim, hidden_dim, max_sent_length, embedding_matrix ):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_sent_length,weights = [embedding_matrix], trainable=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(int(hidden_dim / 2))))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= METRICS)\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model_list = {'LSTM': lstm, 'CNN': CNN, 'BI_LSTM':bi_lstm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "8usMi8hX9ByG"
   },
   "source": [
    "#### Stop by Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ak0K3_zT9ByG"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "JgZ0Be-q9ByI"
   },
   "outputs": [],
   "source": [
    "model_list = {'CNN': CNN, 'BI_LSTM':bi_lstm, 'LSTM': lstm, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "mPRF0qAR9ByK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list = {'CNN': CNN }\n",
    "my_models = []\n",
    "histories = []\n",
    "for model_name, model in model_list.items():\n",
    "    keras.backend.clear_session()\n",
    "    my_model = model(vocabulary_size3, embedding_dim, hidden_dim, max_sent_length3, embedding_matrix3 )\n",
    "    #weight_dict = compute_class_weight(y_train3)  # there is a typo in version 1: should be y_train3\n",
    "    history = my_model.fit(X_cod_train3, y_cat_train3, validation_data = (X_cod_dev3, y_cat_dev3 ), epochs = n_epoches, callbacks=[es],\n",
    "                            sample_weight = compute_sample_weight('balanced',y_cat_train3),verbose = 1)\n",
    "    my_models += [my_model]\n",
    "    histories += [history]\n",
    "    bias_var_tradeoff_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "DVkqEJEa9ByM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perf = []\n",
    "for my_model, history, model_name in zip(my_models, histories, model_list.keys()):\n",
    "    train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 = model_eval(my_model, history, X_cod_train3, y_train3, X_cod_dev3, y_dev3, X_cod_test3, y_cat_test3, y_test3, model_name = model_name)\n",
    "    perf += [[model_name, train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 ]]\n",
    "perf = pd.DataFrame(perf, columns = ['model_name','train_acc','train_f1','val_acc','val_f1','test_acc','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "hCtOsztH9ByQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list = {'BI_LSTM':bi_lstm,}\n",
    "my_models = []\n",
    "histories = []\n",
    "for model_name, model in model_list.items():\n",
    "    keras.backend.clear_session()\n",
    "    my_model = model(vocabulary_size3, embedding_dim, hidden_dim, max_sent_length3, embedding_matrix3 )\n",
    "    #weight_dict = compute_class_weight(y_train3)  # there is a typo in version 1: should be y_train3\n",
    "    history = my_model.fit(X_cod_train3, y_cat_train3, validation_data = (X_cod_dev3, y_cat_dev3 ), epochs = n_epoches, callbacks=[es],\n",
    "                            sample_weight = compute_sample_weight('balanced',y_cat_train3),verbose = 1)\n",
    "    my_models += [my_model]\n",
    "    histories += [history]\n",
    "    bias_var_tradeoff_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "NjQvZ29K9ByR"
   },
   "outputs": [],
   "source": [
    "perf = []\n",
    "for my_model, history, model_name in zip(my_models, histories, model_list.keys()):\n",
    "    train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 = model_eval(my_model, history, X_cod_train3, y_train3, X_cod_dev3, y_dev3, X_cod_test3, y_cat_test3, y_test3, model_name = model_name)\n",
    "    perf += [[model_name, train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 ]]\n",
    "perf = pd.DataFrame(perf, columns = ['model_name','train_acc','train_f1','val_acc','val_f1','test_acc','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "n1R-6o_Q9ByS"
   },
   "outputs": [],
   "source": [
    "tmp_perf = pd.read_csv('perf.csv', index_col = 0)\n",
    "tmp_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "qDU1ZhI19ByU"
   },
   "outputs": [],
   "source": [
    "pd.concat([tmp_perf, perf]).to_csv('perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yqekw3vi9ByV"
   },
   "outputs": [],
   "source": [
    "performance_compare_plot(tmp_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjrQGAb99ByX"
   },
   "source": [
    "#### Stop by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A28TpaQs9ByX"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9lktWzu9ByZ",
    "outputId": "4f501dc7-27bc-4af9-9132-fa7a82bae910"
   },
   "outputs": [],
   "source": [
    "model_list = {'CNN_auc': CNN }\n",
    "my_models = []\n",
    "histories = []\n",
    "for model_name, model in model_list.items():\n",
    "    #keras.backend.clear_session()\n",
    "    my_model = model(vocabulary_size3, embedding_dim, hidden_dim, max_sent_length3, embedding_matrix3 )\n",
    "    #weight_dict = compute_class_weight(y_train3)  # there is a typo in version 1: should be y_train3\n",
    "    history = my_model.fit(X_cod_train, y_cat_train, validation_data = (X_cod_dev, y_cat_dev ), epochs = n_epoches, callbacks=[es],\n",
    "                            sample_weight = compute_sample_weight('balanced',y_cat_train),verbose = 1)\n",
    "    my_models += [my_model]\n",
    "    histories += [history]\n",
    "    bias_var_tradeoff_plot(history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rl_JdJLp9Byc",
    "outputId": "56d57e32-b321-4f8c-a064-9f868d815bee"
   },
   "outputs": [],
   "source": [
    "perf = []\n",
    "for my_model, history, model_name in zip(my_models, histories, model_list.keys()):\n",
    "    train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 = model_eval(my_model, history, X_cod_train, y_train, X_cod_dev, y_dev, X_cod_test, y_cat_test, y_test, model_name = model_name)\n",
    "    perf += [[model_name, train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 ]]\n",
    "perf = pd.DataFrame(perf, columns = ['model_name','train_acc','train_f1','val_acc','val_f1','test_acc','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8AN3sDeo9Bye",
    "outputId": "c2f23a61-0551-4635-943b-026dee6d14fe"
   },
   "outputs": [],
   "source": [
    "tmp_perf = pd.read_csv('perf.csv', index_col = 0)\n",
    "tmp_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BY3AbFn69Byi"
   },
   "outputs": [],
   "source": [
    "pd.concat([tmp_perf, perf]).to_csv('perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOc42DUu9Byl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance_compare_plot(tmp_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvS83S329Byo",
    "outputId": "97ae3ae4-0f36-4276-ee37-22568cc204cf"
   },
   "outputs": [],
   "source": [
    "model_list = {'BI_LSTM_auc': bi_lstm }\n",
    "my_models = []\n",
    "histories = []\n",
    "for model_name, model in model_list.items():\n",
    "    #keras.backend.clear_session()\n",
    "    my_model = model(vocabulary_size3, embedding_dim, hidden_dim, max_sent_length3, embedding_matrix3 )\n",
    "    #weight_dict = compute_class_weight(y_train3)  # there is a typo in version 1: should be y_train3\n",
    "    history = my_model.fit(X_cod_train, y_cat_train, validation_data = (X_cod_dev, y_cat_dev ), epochs = n_epoches, callbacks=[es],\n",
    "                            sample_weight = compute_sample_weight('balanced',y_cat_train),verbose = 1)\n",
    "    my_models += [my_model]\n",
    "    histories += [history]\n",
    "    bias_var_tradeoff_plot(history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqSWEfFf9Byq",
    "outputId": "a1cf6bcd-335c-4795-bc16-23cc6719ded9"
   },
   "outputs": [],
   "source": [
    "perf = []\n",
    "for my_model, history, model_name in zip(my_models, histories, model_list.keys()):\n",
    "    train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 = model_eval(my_model, history, X_cod_train, y_train, X_cod_dev, y_dev, X_cod_test, y_cat_test, y_test, model_name = model_name)\n",
    "    perf += [[model_name, train_acc, train_f1 , val_acc, val_f1 , test_acc, test_f1 ]]\n",
    "perf = pd.DataFrame(perf, columns = ['model_name','train_acc','train_f1','val_acc','val_f1','test_acc','test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVArp2cb9Bys",
    "outputId": "09b25b89-d1e7-4983-fa14-0113b870d2df"
   },
   "outputs": [],
   "source": [
    "tmp_perf = pd.read_csv('perf.csv', index_col = 0)\n",
    "tmp_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9p_1ipIA9Byu",
    "outputId": "25edbff3-7ac7-4f5f-c0af-dc8695052eb0"
   },
   "outputs": [],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6P8xSIk9Byv"
   },
   "outputs": [],
   "source": [
    "pd.concat([tmp_perf, perf]).to_csv('perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzKf8vG69Byw",
    "outputId": "ae6ba28e-5dff-4b51-a92d-6c2304d28626"
   },
   "outputs": [],
   "source": [
    "performance_compare_plot(tmp_perf[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX4gBXt79Byx",
    "outputId": "9385316c-d8d8-48f1-8b19-0d3b8f54ae6a"
   },
   "outputs": [],
   "source": [
    "tmp_perf = tmp_perf.sort_values('test_f1', ascending = False)\n",
    "#.style.hightlight_max(axis = 0, color = 'green')\n",
    "tmp_perf.set_index('model_name').style.highlight_max( axis = 0, color = 'green')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "preliminary_models_midterm_present.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
